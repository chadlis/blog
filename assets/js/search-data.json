{
  
    
        "post0": {
            "title": "AI Fairness",
            "content": "Introduction . The concept of equity is implicitly learned by humans since childhood. This results in an approximate and subjective understanding that may emphasize different aspects and considerations: impartiality, merit, etc. . Machine Learning techniques are increasingly used in decision-making contexts with important societal implications. These automated decision-making mechanisms act under explicit or implicit conceptions of equity. . In some cases, a decision based on irrelevant elements can be characterized as biased (for example, ethnicity or gender in a hiring process), but this can also be the case for a completely arbitrary decision in other contexts (for example, giving the trophy to a random player). . A list of artificial intelligence projects showing these drifts has emerged and is growing month by month, ranging from a racist chatbot to a sexist recruitment algorithm. . One of the best known cases is COMPAS (Correctional Management Profiling for Alternative Sanctions), a decision-support software used in the United States to determine the likelihood that an accused may become a repeat offender. In an article published in 2016, ProPublica demonstrates a blatant bias in COMPAS decision making towards certain populations by systematically attributing a significantly higher risk to them. . The primary objective of this article is to introduce and raise awareness about the complexity of the issue of fairness in artificial intelligence: what is the state of legislation on the issue? How to define and quantify algorithmic biases? And in practice, how to correct them? . European legislation closely monitoring the fairness of machine learning models . Since the implementation of the GDPR (General Regulation on Data Protection) in May 2018, work has been carried out by the European Commission to promote an ecosystem of trust for artificial intelligence. This work was written and published in February 2020 in a white paper “Public consultation towards a European approach for excellence and trust”. The objective is to extend the regulation of GDPR to AI during 2021 by building on the 7 requirements listed in this white paper by encouraging innovation around ethical AI. . This willingness to establish a regulatory framework encourages companies to take the necessary steps to develop fair algorithms, especially those in sectors most prone to discrimination. . Algorithmic biases have a greater impact in certain areas in particular. In the medical field, for example, the use of data collected from certain populations could have serious consequences on under-represented or absent populations. Some studies have already established that many data sets used in the medical field are biased. One example is the 23andme genome dataset, which is said to have only 3% African people. . In the financial sector, equity issues are increasingly emphasized. Recent work by the ACPR (French Prudential Supervision and Resolution Authority) shows that few actors have measured and sought to correct existing biases: “Exploratory work carried out by the ACPR, even when complemented by a more general study of the financial sector, has shown that only a few actors in the financial sector have begun to address the issue of detecting and correcting model biases”. The existence of biases in this sector can also have far-reaching consequences, for example in the granting of loans, a case that is very frequently studied when looking at biases. . How to define algorithmic fairness? . There is no universal definition of fairness today. In the algorithmic framework of decision making, Mehrabi et al. give the following very general definition which includes two essential elements that are discrimination and group distinction : . “Algorithmic fairness refers to the absence of any favouritism or discrimination against an individual or group formed by innate or acquired characteristics.” . It is therefore a matter of verifying and evaluating the absence of any harm, a discrimination that could be caused by decisions made by an algorithm. The challenge is then to precisely define these biases that one wishes to avoid in order to set up metrics to measure and correct them. . Protected / sensitive variables . The groups mentioned in the previous definition can be defined using variables called protected variables or sensitive variables. These variables allow the socio-cultural characterization of each piece of data. Gender, skin color, nationality, religion, family status, age are characteristics explicitly and legally defined as sensitive. . An interesting remark concerning these variables is that there may be other non-sensitive variables linked to these first ones indirectly. There may indeed be a correlation between a sensitive variable and another non-sensitive variable that influences the final decision. For example, gender may be strongly related to occupation. Occupation is not a protected variable, but an imbalance of this variable in the data used could lead to gender bias in the decisions made by the algorithm. . Therefore, what to do with sensitive or indirectly sensitive variables? Removing directly or indirectly sensitive variables when training a Machine Learning model is not necessarily a good strategy. Indeed, an algorithmic bias may still remain at the output of the model. Removing them can even worsen existing biases. . It is necessary to cross-reference the sensitive variables with the decisions of the model in order to be able to identify potential existing biases corresponding to discriminatory decisions made by the model. However, the use of these variables is not necessarily easy when considering of the General Data Protection Regulation (GDPR). . Measures of Algorithmic Bias . There are a multitude of aspects of algorithmic fairness, each of which is associated with mathematical measures to quantify it. . One could imagine a set of metrics forming a universal framework for determining the decision biases introduced by an algorithm, but in reality, combining some of these metrics often proves difficult or even impossible. . Thus, there is no universal measure, nor are some metrics better than others. Each metric must be considered according to the subject matter. For example, when recruiting, the goal is to ensure that a man has exactly the same chance as a woman to be hired. . On the other hand, in the context of automobile insurance, knowing that men have on average more accidents than women, one may want to check that for a given score, we observe the same probability of having actually had an accident for men and women. We thus evaluate groups calibration (defined below). . Thus, it is necessary in the course of each project to identify the risks and issues associated with decision making process of the algorithm, and to deduce the most appropriate metrics. Some of the most commonly used definitions are given below. . For this introductory article, we present only the case of binary classification. In addition to its simplicity, it is also the most studied case in the literature and finds multiple applications (credit granting, hiring, customer scoring, etc.). . In order to clarify these definitions, they are illustrated by the example of the granting of credit, which consists in predicting the probability that a loaned sum will be well repaid by the individual, i.e. calculating the risk associated with credit default. . The following notations are introduced beforehand : . Y: the target class that the algorithm tries to predict. More precisely, in the example of the granting of credit, Y is 1 if the person has repaid the loan and 0 otherwise. . S: the variable representing the score associated with the algorithm’s prediction (value between 0 and 1). In the example, the closer the score is to 0, the higher the risk that the person will not repay the loan. . Ŷ: the decision made by the algorithm. This variable is linked to the score and is equal to 0 if the score is below a defined threshold, or 1 otherwise. In the example, if Ŷ is 0 then the algorithm predicts a credit default. . G: the sensitive variable that defines the groups for which we want to measure the different biases. In the example, G corresponds to the gender of the person. We want to verify that the algorithm does not favor either men or women. . 1. Independence . Les métriques qui cherchent à satisfaire le critère d’independence mesurent l’influence des groupes définis par la variable sensible sur la classe prédite. Parmi ces mesures, on retrouve notamment la définition suivante : Statistical Parity Cette métrique évalue si chaque groupe a la même probabilité d’appartenir à la classe prédite positive. . Statistical parity . This metric evaluates whether each group has the same probability of belonging to the positive predicted class . In the case of credit granting, for example, the model compares the probabilities of being able to repay a loan (given by the model) according to whether one is a man or a woman. If 4 men and 12 women apply for a loan, and if the algorithm decides to grant 8 loans, we want 2 men and 6 women to be selected (2/4 = 6/12). . 2. Separation . The independence criteria do not take into account the target variable Y. Definitions based on the separation criterion take this target variable into account in order to measure the independence between the score obtained by the algorithm and the sensitive variable conditioned by the target variable. These definitions allow G to be explanatory of Y, contrary to the previous definitions which assume that Y is independent of G. They thus take into account a potential influence of the sensitive variable on the target variable. . Equal opportunity . This metric compares the rates of true positives in the different groups. . In our previous example, we thus observe the probabilities that the model predicts credit repayment, depending on whether it is for a man or a woman, knowing that the person has actually repaid the amount borrowed. . We consider a sample of 100 people containing 50 women and 50 men. Of the 50 women, 40 have repaid their loans, and only 20 of the men have repaid their loans. We imagine that among the persons selected by the algorithm, 30 persons have actually repaid their credit (the other persons selected are therefore errors in the algorithm). Then the algorithm satisfies the introduced metric if among these 30 persons, 20 are women and 10 are men (20/40=10/20). . This metric therefore takes into account the information provided by the sensitive variable on the target variable, i.e. in this example men are less reliable than women when granting credit. . 3. Sufficiency . The metrics grouped around the sufficiency criterion seek to measure the independence of the target variable Y with respect to group G conditioned by the score variable S. This family of metrics is incompatible with the equilibrium metrics of the positive and negative classes. . Test fairness or calibration . We test here that the probability of belonging to the positive class is the same for a given score: . In the previous example, we consider a score given by the model, and for all the persons having obtained this score, we compare the probabilities that this person has actually repaid the credit according to whether he is a man or a woman. . And in practice, how can these biases be corrected? . The analysis and correction of algorithmic biases can be integrated into the different levels of a Machine Learning project: pre-processing, in-processing, post-processing. . Pre-processing (data) This step allows to analyze and process the distribution of the model training data (discriminating sensitive variables, class imbalance). The objective is to apply transformations on the input data to mitigate biases and make them more equitable. . In-processing (modeling/optimization) In this step, the main treatments consist in defining equity metrics to be optimized in conjunction with performance metrics during model training. . Post-processing (results) The objective at this stage is to analyze and transform the model results to identify and address potential biases related to protected variables or subgroups related to these variables. . As pre-processing and post-processing methods act only on input data or results, they do not require transparency of the model and can treat it as a black box. In particular, this allows the Machine Learning libraries to be used directly without explicit modification. On the other hand, this kind of processing can make the model less interpretable. . Conclusion . As Machine Learning techniques are increasingly used in important decision-making contexts, it becomes necessary to ensure that this does not generate unforeseen impact. with negative societal implications. These should be specified during the project to identify existing biases in the data in order to derive the right indicators. A multitude of methods exist to correct these biases and can be integrated into the different stages of a Machine Learning project. The choice of these methods depends, among other things, on the metrics chosen and the interpretability constraints. These metrics and correction methods can already be found in open-source python libraries. Nevertheless, there is still a lack of scientific and technological consensus on the choice of the right correction algorithm according to the business use case, while keeping a good fair/performance compromise, even if some have already tried it. . References . https://arxiv.org/pdf/1908.09635.pdf https://github.com/wikistat/Fair-ML-4-Ethical-AI#annexe-extraits-du-guide-des-experts-de-la-ce-pour-une-ia-digne-de-confiance https://papers.nips.cc/paper/2020/file/7ec2442aa04c157590b2fa1a7d093a33-Paper.pdfhttps://arxiv.org/pdf/1609.05807.pdf https://arxiv.org/abs/1609.05807 https://arxiv.org/abs/1908.09635 https://www.quantmetry.com/blog/ia-de-confiance-exigence-et-opportunite-europeenne/ https://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence-feb2020_fr.pdf https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing . Remarque . This is a modified version. This original article is in French. Written by: Salah Chadli, Philippe Neveux, Thibaud Real Del Sarte. .",
            "url": "https://chadlis.github.io/blog/markdown/2021/01/23/ai-fairness.html",
            "relUrl": "/markdown/2021/01/23/ai-fairness.html",
            "date": " • Jan 23, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Anomaly Detection",
            "content": "Introduction . What do your Twitter posts on New Year’s Day, the payment of your Netflix subscription and your last Uber trip have in common? Well, all these actions are monitored in real time via anomaly detection algorithms that ensure that abnormal phenomena are anticipated and that the quality of service is maintained. . Data are generally generated by a specific process reflecting a particular activity: the historical distribution of the amounts and frequencies of bank transactions for a given customer can give indications about his or her usual behavior. . Anomalies are generated by an unusual behavior of the data generation process. Therefore, they often contain useful information on the characteristics of this behaviour: a very high banking transaction (relative to usual transactions) may be indicative of a case of bank fraud. . Anomaly detection consists in finding patterns in the data that deviate from the so-called “normal” behavior. In the literature, there are many denominations referring more or less equally to this task. In this post, we will consider that outlier detection consists in identifying “undesirable” values in order to simply do data cleaning, where anomaly detection tries to identify events of interest in order to detect their cause. . From one field of application to another, the response provided by the anomaly detection system will not be the same. The very notion of anomaly is different from one problem to another. In health, a small deviation from the average can be considered abnormal, whereas such a deviation on financial data on the stock market can be considered usual and common. . Unknown or very rare known anomalies . In the case where known abnormal observations are available, on choice is to reduce part or the whole problem of anomaly detection to a problem of supervised classification. . In reality, if available, labelled data are often rare. It is therefore essential to manage this imbalance. The use of a supervised classifier for anomaly detection, although useful for the identification of known anomaly modes, may subsequently have difficulties in identifying new anomaly types on which it has not been trained. . When known abnormal observations are not available. One is therefore in a completely unsupervised context. In this case, the analysis is mainly based on the assumption that the majority of observations would be generated by a normal process. . Some approaches exploit unsupervised methods to build a model of normal data, and thus to be able to identify anomalies as those that are outside this definition of normality. However, these methods rely heavily on some assumptions and only work if they are respected. . As it is often not difficult to collect labelled normal data in addition to a small number of known abnormal data, it is recommended that this should be exploited as much as possible in learning the boundary between normality and abnormality. One way to make this process effiecent is to use Active Learning techniques. . Distinguish anomalies from noise . In a context of anomaly detection, we seek to identify interesting deviations from the normal behavior. As data are often noisy, an important challenge is to be able to distinguish noise from real anomalies. . For example, in an IoT environment where a large number of low-cost and resource-constrained sensors are deployed, data quality is often affected by high noise, inconsistencies and missing or duplicated data. . In an unsupervised context, noise represents the semantic boundary between normal and anomalous data. Noise can be seen as a form of outlier data that does not meet the criteria of interest to be considered an anomaly. . Noise robust methods can help improve the quality of detection. The main challenge here is that the amount of noise can differ considerably between datasets and can be distributed irregularly. . A critical choice of the data model . Anomaly detection techniques often start from different assumptions about the normal behavior of the data, create a model representing this normal behavior according to these assumptions, and then evaluate the deviation of each data from this constructed model. . The choice of the model is crucial. An incorrect choice can lead to poor detection quality because the data do not follow the initial hypotheses. . For example, a Gaussian mixture model may not work properly, if the data do not match the generative assumptions of the model, or if a sufficient number of data points are not available to learn the parameters of the model. Similarly, a linear regression model may malfunction if there are no linear relationships in the data. . Statistical and probabilistic methods . These methods use historical data to model the expected behavior of a system. When a new observation is received, it is compared to the current model of that system and if it does not match the current model, it is recorded as an anomaly. . Distance-based methods . A measure of distance is defined so that a newly received observation can be compared with those that precede it, on the assumption that a smaller distance would most likely occur from similar mechanisms and would therefore be reported as normal. Conversely, a greater distance would indicate that the observation was generated by a different mechanism and would therefore be reported as abnormal. . Clustering . This approach projects the data into a multidimensional space and uses the density of the resulting groups. Observations that have close and dense groups are reported as normal observations, while those that are further away from or outside these groups are reported as abnormal. . Predictive approaches . A regression model is generated based on recent and longer-term trends in the system that predicts the expected value at some point in the future. When a new observation is received, it is compared to these predicted values and an assessment is made of the accuracy of this prediction, where the observed value and the predicted value vary greatly; this observation is flagged as abnormal. . Ensembling methods . It uses a number of different algorithms to observe each data point and some form of voting mechanism is employed on the results of each method. For example, for IoT data, a set can be constructed from a group of similar or dissimilar models. Often, the use of ensemble techniques can improve the overall success of a detection suite, at the possible expense of model complexity and computation time. . The choice of approach is highly dependent on a number of factors within the monitored data as well as the deployment environment. . The curse of dimensionality . Anomalies often show obvious abnormal characteristics in a low-dimensional space, but become hidden and invisible in a high-dimensional space. . In these spaces, data becomes increasingly sparse and all pairs of data points become almost equidistant from each other. As a result, anomaly scores become less distinguishable from each other. In this case, anomalies are better highlighted in a subspace of original or constructed variables. This approach is called “Subspace Anomaly Detection”. . These methods start from the assumption that the abnormal character of the observations can be highlighted in the unusual local structures of small subspaces, and this deviant behaviour is masked by a full-dimensional analysis. . Conclusion . The problem of anomaly detection has applications in many areas, where it is desirable to identify interesting and unusual events in the underlying generation process. . At the heart of all anomaly detection methods is the creation of a probabilistic, statistical or algorithmic model that characterizes the normal data. Deviations from this model are then used to identify anomalies. . A good knowledge of the underlying data in a specific domain is often crucial to design simple and accurate models that do not overfit the underlying data (overfitting). . References . Outlier Analysis. Second Edition. Charu C. Aggarwal . Anomaly Detection for IoT Time-Series Data: A Survey, Andrew Cook, Goksel Mısırlı, and Zhong Fan, ¨ Senior Member, IEEE . S. K. Bose, B. Kar, M. Roy, P. K. Gopalakrishnan, and A. Basu, “Adepos: anomaly detection based power saving for predictive maintenance using edge computing,” in Proceedings of the 24th Asia and South Pacific Design Automation Conference. ACM, 2019, pp. 597–602. . Deep Learning for Anomaly Detection: A Review Guansong Pang, Chunhua Shen, Longbing Cao, Anton van den Hengel . Robust Anomaly Detection on Unreliable Data Zilong Zhao, Sophie Cerf, Robert Birke, Bogdan Robu, Sara Bouchenak, Sonia Ben Mokhtar, Lydia Y. Che .",
            "url": "https://chadlis.github.io/blog/markdown/2020/09/15/anomaly-detection.html",
            "relUrl": "/markdown/2020/09/15/anomaly-detection.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://chadlis.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://chadlis.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://chadlis.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://chadlis.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}