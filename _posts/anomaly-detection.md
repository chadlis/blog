---
toc: true
layout: post
description: Challenges
categories: [markdown]
title: Anomaly Detection
---
# Introduction

What do your Twitter posts on New Year's Day, the payment of your Netflix subscription and your last Uber trip have in common? Well, all these actions are monitored in real time via anomaly detection algorithms that ensure that abnormal phenomena are anticipated and that the quality of service is maintained.

Data are generally generated by a specific process reflecting a particular activity: the historical distribution of the amounts and frequencies of bank transactions for a given customer can give indications about his or her usual behavior.

Anomalies are generated by an unusual behavior of the data generation process. Therefore, they  often contain useful information on the characteristics of this behaviour: a very high banking transaction (relative to usual transactions) may be indicative of a case of bank fraud. 

Anomaly detection consists in finding patterns in the data that deviate from the so-called "normal" behavior. In the literature, there are many denominations referring more or less equally to this task. In this post, we will consider that outlier detection consists in identifying "undesirable" values in order to simply do data cleaning, where anomaly detection tries to identify events of interest in order to detect their cause.

From one field of application to another, the response provided by the anomaly detection system will not be the same. The very notion of anomaly is different from one problem to another. In health, a small deviation from the average can be considered abnormal, whereas such a deviation on financial data on the stock market can be considered usual and common.

# Unknown or very rare known anomalies

In the case where known abnormal observations are available, on choice is to reduce part or the whole problem of anomaly detection to a problem of supervised classification.

In reality, if available, labelled data are often rare. It is therefore essential to manage this imbalance. The use of a supervised classifier for anomaly detection, although useful for the identification of known anomaly modes, may subsequently have difficulties in identifying new anomaly types on which it has not been trained.

When known abnormal observations are not available. One is therefore in a completely unsupervised context. In this case, the analysis is mainly based on the assumption that the majority of observations would be generated by a normal process.

Some approaches exploit unsupervised methods to build a model of normal data, and thus to be able to identify anomalies as those that are outside this definition of normality. However, these methods rely heavily on some assumptions and only work if they are respected.

As it is often not difficult to collect labelled normal data in addition to a small number of known abnormal data, it is recommended that this should be exploited as much as possible in learning the boundary between normality and abnormality. One way to make this process effiecent is to use Active Learning techniques. 

# Distinguish anomalies from noise

In a context of anomaly detection, we seek to identify interesting deviations from the normal behavior. As data are often noisy, an important challenge is to be able to distinguish noise from real anomalies. 

For example, in an IoT environment where a large number of low-cost and resource-constrained sensors are deployed, data quality is often affected by high noise, inconsistencies and missing or duplicated data.

In an unsupervised context, noise represents the semantic boundary between normal and anomalous data. Noise can be seen as a form of outlier data that does not meet the criteria of interest to be considered an anomaly.

Noise robust methods can help improve the quality of detection. The main challenge here is that the amount of noise can differ considerably between datasets and can be distributed irregularly.


# A critical choice of the data model

Anomaly detection techniques often start from different assumptions about the normal behavior of the data, create a model representing this normal behavior according to these assumptions, and then evaluate the deviation of each data from this constructed model. 

The choice of the model is crucial. An incorrect choice can lead to poor detection quality because the data do not follow the initial hypotheses.

For example, a Gaussian mixture model may not work properly, if the data do not match the generative assumptions of the model, or if a sufficient number of data points are not available to learn the parameters of the model. Similarly, a linear regression model may malfunction if there are no linear relationships in the data.


## Statistical and probabilistic methods
These methods use historical data to model the expected behavior of a system. When a new observation is received, it is compared to the current model of that system and if it does not match the current model, it is recorded as an anomaly.


## Distance-based methods
A measure of distance is defined so that a newly received observation can be compared with those that precede it, on the assumption that a smaller distance would most likely occur from similar mechanisms and would therefore be reported as normal. Conversely, a greater distance would indicate that the observation was generated by a different mechanism and would therefore be reported as abnormal.


## Clustering
This approach projects the data into a multidimensional space and uses the density of the resulting groups. Observations that have close and dense groups are reported as normal observations, while those that are further away from or outside these groups are reported as abnormal.


## Predictive approaches
A regression model is generated based on recent and longer-term trends in the system that predicts the expected value at some point in the future. When a new observation is received, it is compared to these predicted values and an assessment is made of the accuracy of this prediction, where the observed value and the predicted value vary greatly; this observation is flagged as abnormal.

## Ensembling methods
It uses a number of different algorithms to observe each data point and some form of voting mechanism is employed on the results of each method. For example, for IoT data, a set can be constructed from a group of similar or dissimilar models. Often, the use of ensemble techniques can improve the overall success of a detection suite, at the possible expense of model complexity and computation time.

The choice of approach is highly dependent on a number of factors within the monitored data as well as the deployment environment.



## The curse of dimensionality 

Anomalies often show obvious abnormal characteristics in a low-dimensional space, but become hidden and invisible in a high-dimensional space. 

In these spaces, data becomes increasingly sparse and all pairs of data points become almost equidistant from each other. As a result, anomaly scores become less distinguishable from each other.
In this case, anomalies are better highlighted in a subspace of original or constructed variables. This approach is called "Subspace Anomaly Detection". 

These methods start from the assumption that the abnormal character of the observations can be highlighted in the unusual local structures of small subspaces, and this deviant behaviour is masked by a full-dimensional analysis. 


# Conclusion

The problem of anomaly detection has applications in many areas, where it is desirable to identify interesting and unusual events in the underlying generation process.

At the heart of all anomaly detection methods is the creation of a probabilistic, statistical or algorithmic model that characterizes the normal data. Deviations from this model are then used to identify anomalies. 

A good knowledge of the underlying data in a specific domain is often crucial to design simple and accurate models that do not overfit the underlying data (overfitting).

### References 
Outlier Analysis. Second Edition. Charu C. Aggarwal

Anomaly Detection for IoT Time-Series Data: A Survey, Andrew Cook, Goksel Mısırlı, and Zhong Fan, ¨ Senior Member, IEEE

S. K. Bose, B. Kar, M. Roy, P. K. Gopalakrishnan, and A. Basu, “Adepos: anomaly detection based power saving for predictive maintenance
using edge computing,” in Proceedings of the 24th Asia and South
Pacific Design Automation Conference. ACM, 2019, pp. 597–602.

Deep Learning for Anomaly Detection: A Review
Guansong Pang, Chunhua Shen, Longbing Cao, Anton van den Hengel

Robust Anomaly Detection on Unreliable Data Zilong Zhao, Sophie Cerf, Robert Birke, Bogdan Robu, Sara Bouchenak, Sonia Ben Mokhtar, Lydia Y. Che
